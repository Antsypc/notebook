集成学习(Ensemble Learning)
===========================

## 核心概念
在机器学习领域，我们经常使用一些算法对数据进行训练，得出决策模型（学习器），不同的方法得出的结论有好有坏，所以我们就思考能不能结合几个比较好的学习方法，从而得到更好的训练效果呢。

集成学习就是这样一种思想，主要是**组合多个效果好但有差异的学习器**来提升学习效果。所以也称为多分类器系统。

其中，每一个`个体学习器（Individual Learner）`是使用简单的学习算法对数据训练产生的，如决策树算法、神经网络算法等。

- 同质（homogeneous）集成：所有的个体学习器**由同一种学习算法生成**，所以，其学习器也称为`基学习器（Base Learner）`，其个体算法称为`基学习算法（Base Learning Algorithm）`。
- 异质（heterogeneous）集成：所有的个体学习器**由不同学习算法生成**，所以，其学习器也称为`组件学习器（Component Learner）`。

个体学习器如果是`弱学习器`，多个学习器组合后效果提升格外明显。所以，通常把个体学习器直接称作了弱学习器。
PS：弱学习器是指找出隐藏在数据背后的规律的能力（泛化性能），比瞎猜好一丢丢，正确率高于50%，可以直接理解为学习能力比较弱。

## 组合多个学习器提升学习效果的思路
从经验上讲，把好的学习器和不好的学习器组合在一起，效果肯定次于最好的学习器，又强于最差的学习器，那么为什么组合后能达到性能的提升呢？

举个例子，使用多种分类算法对数据进行分类。下面表格中，YES表示正确分类，NO表示分类错误。

|-|数据1|数据2|数据3|
|-|----|-----|----|
|算法1|YES|YES|NO|
|算法2|NO|YES|YES|
|算法3|YES|NO|YES|
|集成|YES|YES|YES|

可以看到，表格中集成就是少数服从多数的策略，也就是**投票法（voting）**产生最终的分类结果。单独看每个分类器，精度只有2/3，但是集成后精度达到100%.

从上面表格也可以看出，对于单个学习器是有要求的。他们必须本身有比较好的分类效果，同时效果还要有所不同。都把一个数据分类错误了，那投票也没用。这就是要求分类器**好而不同**。

假设基分类器的错误率相互独立，根据Hoeffding不等式，随着个体分类器越多，集成分类的错误率就越小。但是误差独立却比较难，因为通常学习器是从同一个问题训练出来的，那么他们就不是独立的。如果做到学习器的准确性和多样性并存是一个研究的点。

## 具体的算法
随着发展，不一定都通过投票法产生集成学习器，也有通过梯度提升（Gradient Boosting）等方式的。

根据个体学习器的生成方式分类

- 个体学习器强依赖、必须串行生成的序列化方法：Boosting, AdaBoost,GBDT(XGBoost,LightGBM).
- 个体学习器不依赖、可同时生成的并行化方法：Bagging,随机森林(Random Forest).
